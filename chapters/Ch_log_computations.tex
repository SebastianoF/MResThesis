\chapter{Log-composition to Compute the Lie Logarithm}\label{ch:log_algorithm}

\begin{flushright}
	I think you might do something better with the time \\ than wasting it in asking riddles that have no answers. \\ -\emph{Alice in Wonderland.}
\end{flushright}

\vspace{0.5 cm}

\noindent
The \emph{logarithm computation problem} can be stated as follows:
\begin{center}
\emph{
	Given $p$ in a Lie group $\mathbb{G}$, \\ 
	what is the element $\mathbf{u}$ in its Lie algebra $\mathfrak{g}$ \\
	such that $\exp(\mathbf{u}) = p$ ?  
}
\end{center}
There are several numerical methods to compute the approximation of the problem's solution. Arsigny, who first pointed the applications of the Lie logarithm in medical image registration in \cite{Arsigny:MRM:06} and \cite{arsigny2006bi}, proposed the Inverse scaling and squaring (see also \cite{ying2006phase}). In this chapter we investigate other numerical iterative algorithms for the computation of the Lie logarithm, called here \emph{logarithm computation algorithm}; they modifications of the algorithm presented in \cite{Bossa:08} that is based on the BCH formula, and so on the log-composition. Each of the numerical method to compute the log-composition become naturally a numerical method for the computation of the logarithm computation algorithm.\\
The first step toward this direction is to introduce the space of the approximations of a Lie algebra and a the Lie group.


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% % SUBSECTION
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{Spaces of Approximations}\label{se:space_of_approximation}

\noindent
As seen in section \ref{se:rigid_body_transformations} and \ref{se:svf}, if the element $\mathbf{u}$ of $\mathfrak{se}(2)$ or SVF is small enough we can approximate $\exp(\mathbf{u})$ with $e + \mathbf{u}$. Aim of this section is to investigate these approximations aimed to compute the logarithm.  \\
Let $C_\mathfrak{g}$ and $C_\mathbb{G}$ the internal cut locus of a Lie algebra and a Lie group $\mathfrak{g}$ and $\mathbb{G}$ (the subset where the exponential map is well defined).
We define two approximating functions:
\begin{align*}
\text{app} : C_\mathfrak{g} & \longrightarrow  C_\mathfrak{g}^{\sim}    \\
\mathbf{u} &\longmapsto \exp(\mathbf{u}) - e
\end{align*}
\begin{align*}
\text{App} : C_\mathbb{G} & \longrightarrow  C_\mathbb{G}^{\sim}   \\
\exp(\mathbf{u}) &\longmapsto e + \mathbf{u}
\end{align*}
Where $C_\mathfrak{g} ^{\sim}$ is the space of approximations of elements of $C_\mathfrak{g} $, and $C_\mathbb{G}^{\sim} $ is the space of approximations of elements in $C_\mathbb{G}$, defined as
\begin{align*}
C_\mathfrak{g} ^{\sim} & := \{ \exp(\mathbf{u}) - e \mid \mathbf{u}\in C_\mathfrak{g}\} \cup C_\mathfrak{g} \\
C_\mathbb{G}^{\sim}  & := \{ e + \mathbf{u} \mid \mathbf{u}\in C_\mathbb{G}\} \cup C_\mathbb{G} \\
\end{align*}
In general $C_\mathfrak{g}^{\sim} \neq C_\mathfrak{g}$ and $C_\mathbb{G}^{\sim} \neq C_\mathbb{G}$, but in the considered cases of $\mathfrak{se}(2)$ and SVF, when $\mathbf{u}$ is \emph{small enough}
it follows that $\exp(\mathbf{u}) - e \in C_\mathfrak{g} $ and $e + \mathbf{u}\in C_\mathbb{G}$. Therefore the elements of $C_\mathfrak{g}^{\sim} $ are compatible with all of the operations of the internal cut locus of the Lie algebra $C_\mathfrak{g}$ and the elements of $C_\mathbb{G}^{\sim}$ are compatible with all of the operations of Lie group $\mathbb{G}$.\\
Lets examine what does \emph{small enough} means in these two cases:
\begin{enumerate}
	\item[$\mathfrak{se}(2)$ -] Since $\mathfrak{se}(2)$ and $SE(2)$ are subset of the bigger algebra $SE(2)$ then exp and log can be defined as infinite series. From 
	\begin{align*}
	\exp(\mathbf{u}) = I + \mathbf{u} + O(\mathbf{u}^2) 
	\end{align*}
	It follows that $\text{app}(\mathbf{u}) - \mathbf{u} = O(\mathbf{u}^2)$. Thus for all $\mathbf{u}$ in the internal cut locus smaller than $\delta$ for any norm, exists $M(\delta)$ such that
	\begin{align*}
	\euclideanMetric{\text{app}(\mathbf{u}) - \mathbf{u} } < M(\delta) \euclideanMetric{\mathbf{u}^2}
	\end{align*}
	\item[SVF -] In case of SVF we do not have any Taylor series and big-O notation available but, according to the proposition 8.6 at page 163 of \cite{younes2010shapes}, if $\mathbf{u}$ is, for any norm, smaller than $\epsilon<1/C$, where $C$ is the Lipschitz constant in the same norm, then $1 + \mathbf{u}$ is a diffeomorphism. With this condition holds that
	$C_\text{SVF}^{\sim} = C_\text{SVF}$. 
\end{enumerate}

\noindent
Therefore, for each small enough $\mathbf{u}$ in $\mathfrak{se}(2)$ or SVF, 
and for the definition of the log-composition (equation \ref{eq:main_def_log_composition}) 
the following properties holds:
\begin{enumerate}
	\item The approximations $\mathbf{u} \simeq   \text{app} (\mathbf{u})$, $\exp(\mathbf{u}) \simeq   \text{App} (\exp(\mathbf{u})) $ are bounded.
	\item $\mathbf{u} = \mathbf{v} \oplus (-\mathbf{v} \oplus  \mathbf{u} )$
	\item $\text{app} (\mathbf{v} \oplus  \mathbf{u}) = \exp(\mathbf{v})\circ\exp(\mathbf{u}) - 1 \in C_\mathfrak{g} ^{\sim}$
\end{enumerate}

\noindent
With this machinery, we can finally reformulate the algorithm presented in \cite{Bossa:08} for the numerical computation of the Lie logarithm map using the log-composition.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% % SUBSECTION
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{The Logarithm Computation Algorithm using Log-composition}

If the goal is to find $\mathbf{u}$ when its exponential is known, we can consider the sequence transformations $\{\mathbf{u}_{j}  \}_{j=0}^{\infty}$ that approximate $\mathbf{u}$ as consequence of
\begin{align*}
\mathbf{u} = \mathbf{u}_{j} \oplus  (-\mathbf{u}_{j}  \oplus  \mathbf{u} ) \Longrightarrow
\mathbf{u} \simeq \mathbf{u}_{j} \oplus  \text{app}(-\mathbf{u}_{j}  \oplus  \mathbf{u} )
\end{align*}
This suggest that a reasonable approximation for the $(j+1)$-th element of the series can be defined by
\begin{align*}
\mathbf{u}_{j+1} & :=  \mathbf{u}_{j} \oplus  \text{app}(-\mathbf{u}_{j}  \oplus  \mathbf{u} )
\end{align*}
If we chose the initial value $\mathbf{u}_{0}$ to be zero, then the algorithm presented in \cite{Bossa:08}  become:
%\begin{align*}
%p= \exp(\mathbf{v}) &= (\exp(\mathbf{v})\circ \exp(-\mathbf{v}))\circ \exp(\mathbf{v})\\
%&= \exp(\mathbf{v})\circ (\exp(-\mathbf{v})\circ p)\\
%&= \exp(\mathbf{v})\circ \exp(\delta \mathbf{v})\\
%&\approx \exp(\mathbf{v})\circ \exp(\tilde{\delta} \mathbf{v})
%\end{align*}
\begin{equation}\label{eq:bossa_reformulated}
\begin{cases}
\mathbf{u}_0 = 0 \\
\mathbf{u}_{j+1} = \mathbf{u}_{j} \oplus  \text{app}(-\mathbf{u}_{j}  \oplus  \mathbf{u} )
\end{cases}
\end{equation}
Making explicit the log-computaiton and the approximation, follows:
\begin{align}
\mathbf{u}_{j+1} 
&=
\mathbf{u}_{j} \oplus \big( \exp(-\mathbf{u}_{j})  \circ   \exp(\mathbf{u}) - e \big)\\
&=
 \log\Big( \exp( \mathbf{u}_{j}) \circ \exp \big( \exp(-\mathbf{u}_{j})  \circ  \varphi - e \big) \Big)
\end{align}
where $\exp(\mathbf{u}) = \varphi$ is given by the problem, and $\mathbf{u}_{j}$ by the previous step. The BCH provides the exact solution of the second member, while strategy that we have examined to compute the log-composition, become a numerical method for the computation of the logarithm.
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% % SUBSECTION
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\subsection{Truncated BCH Strategy}
At each step, we compute the approximation $\mathbf{v}_{j+1}$ with the $k$-th truncation of the BCH formula. The compact form of the algorithm is given by:
\begin{equation}\label{eq:bossa_bch_strat}
\begin{cases}
\mathbf{u}_0 = 0 \\
\mathbf{u}_{j+1} = \text{BCH}^{k}(\mathbf{u}_{j}, \text{app}(-\mathbf{u}_{j}  \oplus  \mathbf{u} ))
\end{cases}
\end{equation}

For $k = 0$, the approximation $\mathbf{u}_{j+1}$ results simply the sum of the two vectors $\mathbf{u}_{j}$ and $ \text{app}(-\mathbf{u}_{j}  \oplus  \mathbf{u} )$:
\begin{align*}
\text{BCH}^{0}(\mathbf{u}_{j}, \text{app}(-\mathbf{u}_{j}  \oplus  \mathbf{u} ))
&=
\mathbf{u}_{j} + \text{app}(-\mathbf{u}_{j}  \oplus  \mathbf{u} )\\
&=
\mathbf{u}_{j} + \exp(-\mathbf{u}_{j})\circ \varphi  - e 
\end{align*}
When $k=1$, it results 
\begin{align*}
\text{BCH}^{1}(\mathbf{u}_{j}, \text{app}(-\mathbf{u}_{j}  \oplus  \mathbf{u} ))
&=
\mathbf{u}_{j} +  \text{app}(-\mathbf{u}_{j}  \oplus  \mathbf{u} ) + \frac{1}{2}[\mathbf{u}_{j},  \text{app}(-\mathbf{u}_{j}  \oplus  \mathbf{u} )]\\
&=
\mathbf{u}_{j} + \exp(-\mathbf{u}_{j})\circ \varphi  - e + \\
&+ \frac{1}{2}(  \mathbf{u}_{j}\cdot( \exp(-\mathbf{u}_{j})\circ \varphi  - e) -  ( \exp(-\mathbf{u}_{j})\circ \varphi - e)\cdot\mathbf{u}_{j})
\end{align*}
And for $k=2$ it become
\begin{align*}
\text{BCH}^{2}(\mathbf{u}_{j}, \text{app}(-\mathbf{u}_{j}  \oplus  \mathbf{u} ))
&=
\mathbf{u}_{j} +  \text{app}(-\mathbf{u}_{j}  \oplus  \mathbf{u} ) 
+ \frac{1}{2}[\mathbf{u}_{j},  \text{app}(-\mathbf{u}_{j}  \oplus  \mathbf{u} )] + \\
&+ \frac{1}{12}\Big([\mathbf{u}_{j},  [\mathbf{u}_{j},  \text{app}(-\mathbf{u}_{j}  \oplus  \mathbf{u} )]] +\\
&+ [\text{app}(-\mathbf{u}_{j}  \oplus  \mathbf{u} ),  [\text{app}(-\mathbf{u}_{j}  \oplus  \mathbf{u} ) ,\mathbf{u}_{j}  ]] \Big)\\
&=
\mathbf{u}_{j} + \exp(-\mathbf{u}_{j})\circ \varphi  - e 
+ \frac{1}{2}[\mathbf{u}_{j},  \exp(-\mathbf{u}_{j})\circ \varphi  - e ] + \\
&+ \frac{1}{12}\Big([\mathbf{u}_{j},  [\mathbf{u}_{j},  \exp(-\mathbf{u}_{j}) \circ\varphi  - e ]]+\\
&+ [\exp(-\mathbf{u}_{j})\circ \varphi  - e ,  [\exp(-\mathbf{u}_{j}) \circ\varphi - e  ,\mathbf{u}_{j}  ]] \Big)\\
\end{align*}

When considering $k = \infty$ and so, the theoretical BCH formula, the following theorem, presented in \cite{Bossa:08}, provides an error bound:
\begin{theorem}[Bossa]\label{th:bossa}
	The iterative algorithm 
	\begin{equation}
	\begin{cases}
	\mathbf{u}_0 = 0 \\
	\mathbf{u}_{j+1} %= \text{BCH}(\mathbf{u}_{j}, \text{app}(-\mathbf{u}_{j}  \oplus  \mathbf{u} ))
	                          = \mathbf{u}_{j} \oplus  \text{app}(-\mathbf{u}_{j}  \oplus  \mathbf{u} )
	\end{cases}
	\end{equation}
	converges to $\mathbf{v}$ with error $\delta_n \in \mathbb{G}$, where
	\begin{align*}
	\delta_{n} := \log(\exp(\mathbf{v})\circ \exp(-\mathbf{v}_{n})) \in O(\euclideanMetric{p - e}^{2^{n}})
	\end{align*}
\end{theorem}

We observe that this upper limit can be computed only when a closed-form for the log-composition is available, as for example $\mathfrak{se}(2)$.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% % SUBSECTION
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\subsection{Parallel Transport Strategy}

If we apply the parallel transport method for the computation of the log-composition, we obtain another version of the logarithm computation algorithm:
\begin{equation}\label{eq:bossa_parallel_strategy}
\begin{cases}
\mathbf{u}_0 = \mathbf{0} \\
\mathbf{u}_{j+1} 
= 
\mathbf{u}_{j} 
+ 
\exp(\frac{\mathbf{u}_{j}}{2}) \circ \exp\Big(   \text{app}(-\mathbf{u}_{j}  \oplus  \mathbf{u} ) \Big)
\circ 
\exp(-\frac{\mathbf{u}_{j}}{2}) - e
\end{cases}
\end{equation}
That is computed as:
\begin{align*}
\mathbf{u}_{j+1} 
= 
\mathbf{u}_{j} 
+ 
\exp(\frac{\mathbf{u}_{j}}{2}) \circ \exp\Big(   \exp(-\mathbf{u}_{j}) \circ\varphi  - e \Big)
\circ 
\exp(-\frac{\mathbf{u}_{j}}{2}) - e
\end{align*}

We notice that mixing the operation of composition, sum and scalar product makes sense when the involved vectors are \emph{small enough}, as stated in \ref{se:space_of_approximation}. 
Analytical computation of an upper bound error is not straightforward in this case. See section \ref{se:conclusions} for further details and other possible researches.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% % SUBSECTION
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\subsection{Symmetrization Strategy}
The algorithm \ref{eq:bossa_reformulated} could have been reformulated alternatively as $\mathbf{u}_{j+1} =    \text{app}(\mathbf{u} \oplus  - \mathbf{u}_{j}  ) \oplus \mathbf{u}_{j}$. The log-composition is not symmetric therefore the two version in some cases may not return the same value. In an attempt to move toward the solution of this issue we consider
\begin{equation}\label{eq:bossa_symmetric}
\begin{cases}
\mathbf{u}_0 = 0 \\
\mathbf{u}_{j+1} = \mathbf{u}_{j} \oplus 
\frac{1}{2}
\big(  
\text{app}(-\mathbf{u}_{j}  \oplus  \mathbf{u} )
+
\text{app}(\mathbf{u} \oplus  - \mathbf{u}_{j}  )
\big)
\end{cases}
\end{equation}
Writing directly the approximations and using the BCH approximation of degree $1$ it become:
\begin{equation}
\begin{cases}
\mathbf{u}_0 = 0 \\
\mathbf{u}_{j+1} 
%= \mathbf{u}_{j} +  
%\frac{1}{2}
%\big(  
%\text{app}(-\mathbf{u}_{j}  \oplus  \mathbf{u} )
%+
%\text{app}(\mathbf{u} \oplus  - \mathbf{u}_{j}  )
%\big)
=
\mathbf{u}_{j} +  
\frac{1}{2}
\big(  
\exp(-\mathbf{u}_{j}) \circ \varphi ) - e
+
\varphi\circ\exp(-\mathbf{u}_{j}) - e
\big)
\end{cases}
\end{equation}
Experimental results of the methods presented in this section are presented in the next chapter.

%
%The algorithm for the computation of the group logarithm can be improved considering a symmetric version of the underpinning strategy. In this version we use the first order approximation of the BCH formula (see equation (\ref{eq:first_order_approx}) in the following proof), compensating with the fact that the symmetrization should decrease the error involved.
%It gives birth to the following algorithm:
%\begin{equation}\label{eq:sym_strategy}
%\begin{cases}
%\mathbf{v}_0 = \mathbf{0} \\
%\mathbf{v}_{t+1} = \mathbf{v}_{t} + \frac{1}{2}(\tilde{\delta} \mathbf{v}^{L}_{t} +\tilde{\delta} \mathbf{v}^{R}_{t})
%\end{cases}
%\end{equation}
%Where $\tilde{\delta} \mathbf{v}^{R}_{t} = \exp(\mathbf{v})\circ \exp(- \mathbf{v}_{t}) - e$ and $\tilde{\delta} \mathbf{v}^{L}_{t} = \exp(-\mathbf{v}_{t})\circ \exp(\mathbf{v}) - e$.\\
%\begin{proof}
%	To show why it works we remind that the starting point was
%	\begin{align*}
%	p= \exp(\mathbf{v}) &=  \exp(\mathbf{v}_{0})\circ \exp(\delta \mathbf{v}_{0})
%	\end{align*}
%	where $\exp(\delta \mathbf{v}_{0}) = \exp(-\mathbf{v}_{0})\circ p$.\\
%	An equivalent starting point would have been $\exp(\mathbf{v}) = \exp(\delta \mathbf{v})\circ \exp(\mathbf{v}_{0})$ for $\exp(\delta \mathbf{v}) = p\circ \exp(-\mathbf{v}_{0})$. \\
%	This idea leads to the definition of
%	\begin{align*}
%	\exp(\delta \mathbf{v}^{R}_{t}) &:= p\circ \exp(- \mathbf{v}_{t}) = \exp(\mathbf{v})\circ \exp(- \mathbf{v}_{t})\\
%	\exp(\delta \mathbf{v}^{L}_{t}) &:=  \exp(- \mathbf{v}_{t}) \circ p = \exp(- \mathbf{v}_{t}) \circ \exp(\mathbf{v}) 
%	\end{align*}
%	It follows that 
%	\begin{align*}
%	\exp(\mathbf{v}) &= \exp(\mathbf{v}_{0})\circ \exp(\delta \mathbf{v}^{R}_{0})\\
%	\exp(\mathbf{v}) &=  \exp(\delta \mathbf{v}^{L}_{0}) \circ \exp(\mathbf{v}_{0})
%	\end{align*}
%	Using $\exp(\delta \mathbf{v}^{R}_{t}) \approx e + \delta \mathbf{v}^{R}_{t}$ and $\exp(\delta \mathbf{v}^{L}_{t}) \approx e + \delta \mathbf{v}^{L}_{t}$ we can use the following approximation to define the symmetric algorithm:
%	\begin{align*}
%	\exp(\delta \mathbf{v}^{R}_{t}) &= \exp(\mathbf{v})\circ \exp(-\mathbf{v}_{t})\\
%	e + \tilde{\delta} \mathbf{v}^{R}_{t} &= \exp(\mathbf{v})\circ \exp(- \mathbf{v}_{t})\\
%	\tilde{\delta} \mathbf{v}^{R}_{t} &= \exp(\mathbf{v})\circ \exp(- \mathbf{v}_{t}) - e
%	\end{align*}
%	\begin{align*}
%	\exp(\delta \mathbf{v}^{L}_{t}) &= \exp(- \mathbf{v}_{t}) \circ \exp(\mathbf{v})\\
%	e + \tilde{\delta} \mathbf{v}^{L}_{t} &= \exp(-\mathbf{v}_{t})\circ \exp( \mathbf{v})\\
%	\tilde{\delta} \mathbf{v}^{L}_{t} &= \exp(-\mathbf{v}_{t})\circ \exp(\mathbf{v}) - e
%	\end{align*}
%	Which gives birth to iterative algorithm, for a given initial value $V_0$:  
%	\begin{equation}
%	\begin{cases}
%	\mathbf{v}_0  \\
%	\mathbf{v}_{t+1} =\text{BCH}(\mathbf{v}_{t},\tilde{\delta} \mathbf{v}^{R}_{t})
%	\end{cases}
%	\begin{cases}
%	\mathbf{v}_0  \\
%	\mathbf{v}_{t+1} = \text{BCH}(\tilde{\delta} \mathbf{v}^{L}_{t}, \mathbf{v}_{t})
%	\end{cases}
%	\end{equation}
%	If follows that
%	\begin{align*}
%	\mathbf{v}_{t+1} = \frac{1}{2}(\text{BCH}(\tilde{\delta} \mathbf{v}^{L}_{t}, \mathbf{v}_{t}) + \text{BCH}(\mathbf{v}_{t},\tilde{\delta} \mathbf{v}^{R}_{t}))
%	\end{align*}
%	Taking the first order approximation of the BCH formula:
%	\begin{align}\label{eq:first_order_approx}
%	BCH(\tilde{\delta} \mathbf{v}^{L}_{t}, \mathbf{v}_{t}) &\approx \tilde{\delta} \mathbf{v}^{L}_{t} + \mathbf{v}_{t}\\
%	BCH(\mathbf{v}_{t},\tilde{\delta} \mathbf{v}^{R}_{t}) &\approx \mathbf{v}_{t} + \tilde{\delta} \mathbf{v}^{R}_{t}
%	\end{align}
%	we get
%	\begin{align*}
%	\mathbf{v}_{t+1} = \mathbf{v}_{t} + \frac{1}{2}(\tilde{\delta} \mathbf{v}^{L}_{t} + \tilde{\delta} \mathbf{v}^{R}_{t})
%	\end{align*}
%\end{proof}
%We observe that the symmetric approach do not requires to use the BCH formula at each passage, having considered the approximation at the first order of the BCH.\\
%We conclude with a formula that relates $\tilde{\delta} \mathbf{v}^{L}_{t}$ with $\tilde{\delta} \mathbf{v}^{R}_{t}$:
%\begin{theorem}
%	Be $\tilde{\delta} \mathbf{v}^{R}_{t} = \exp(\mathbf{v})\circ \exp(- \mathbf{v}_{t}) - e$ and $\tilde{\delta} \mathbf{v}^{L}_{t} = \exp(-\mathbf{v}_{t})\circ \exp(\mathbf{v}) - e$ as before, then
%	\begin{align*}
%	\delta \mathbf{v}^{L}_{t} \approx \exp(-\mathbf{v}_{t}) \circ \delta \mathbf{v}^{R}_{t} \circ \exp(\mathbf{v}_{t})
%	\end{align*}
%\end{theorem}
%\begin{proof}
%	Since $\exp(\mathbf{v}_{t})\circ \exp(\delta \mathbf{v}^{R}_{t}) \approx exp(\delta \mathbf{v}^{L}_{t}) \circ \exp(\mathbf{v}_{t})$ it follows
%	\begin{align*}
%	\exp(\delta \mathbf{v}^{R}_{t}) = \exp(-\mathbf{v}_{t})\circ \delta \mathbf{v}^{L}_{t} \circ \exp(\mathbf{v}_{t})
%	\end{align*}
%	Using $\exp(\delta \mathbf{v}^{R}_{t}) = e + \delta \mathbf{v}^{R}_{t}$ and $\exp(\delta \mathbf{v}^{L}_{t}) = e + \delta \mathbf{v}^{L}_{t}$ we get
%	\begin{align*}
%	e + \delta \mathbf{v}^{R}_{t} &= \exp(-\mathbf{v}_{t})\circ (e + \delta \mathbf{v}^{L}_{t}) \circ \exp(\mathbf{v}_{t})\\
%	\delta \mathbf{v}^{R}_{t} &= \exp(-\mathbf{v}_{t})\circ \delta \mathbf{v}^{L}_{t} \circ \exp(\mathbf{v}_{t})
%	\end{align*}
%\end{proof}
%
%