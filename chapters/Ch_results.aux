\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{jack2008alzheimer}
\citation{hunter2007}
\citation{scipy}
\citation{modat2010fast}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Experimental Results}{35}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:results}{{5}{35}{Experimental Results}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Log-composition for $\mathfrak  {se}(2)$}{35}{section.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Methods and Results}{35}{subsection.5.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Comparison of the errors for each numerical method to compute the Log-composition $dr_{0} \oplus dr_{1}$ in $\mathfrak  {se}(2)$. Truncated BCH of degrees 0,1,2, parallel transport method and Taylor method are considered for different values of the norm of $dr_{1}$ (x-axes) and norm of $dr_{0}$ (y-axes). The value of each square corresponds to the average error of 500 random samples in each of the 6 sub-intervals between $0.1$ and $2.0$. As expected errors increases with the norm for all of the methods. Errors with BCH 0 and parallel transport method are comparable, but the parallel transport method is not symmetric and has better performance when $dr_{1}$ is small. BCH 1 and Taylor are comparable as well, and they are both symmetric, but the best performance is provided by the BCH 2. Details of the value under the \emph  {gray arrows} are shown in the box-plot \ref  {fig:se2_image_scale} where means, variance, range, quartiles and outliers are visualized. }}{36}{figure.5.1}}
\newlabel{fig:se2_image_scale}{{5.1}{36}{Comparison of the errors for each numerical method to compute the Log-composition $dr_{0} \oplus dr_{1}$ in $\mathfrak {se}(2)$. Truncated BCH of degrees 0,1,2, parallel transport method and Taylor method are considered for different values of the norm of $dr_{1}$ (x-axes) and norm of $dr_{0}$ (y-axes). The value of each square corresponds to the average error of 500 random samples in each of the 6 sub-intervals between $0.1$ and $2.0$. As expected errors increases with the norm for all of the methods. Errors with BCH 0 and parallel transport method are comparable, but the parallel transport method is not symmetric and has better performance when $dr_{1}$ is small. BCH 1 and Taylor are comparable as well, and they are both symmetric, but the best performance is provided by the BCH 2. Details of the value under the \emph {gray arrows} are shown in the box-plot \ref {fig:se2_image_scale} where means, variance, range, quartiles and outliers are visualized}{figure.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Errors of the numerical methods for the computation of the Log-composition of $dr_{0} \oplus dr_{1}$ in $\mathfrak  {se}(2)$. Norm of $dr_{0}$ is in the interval $[0.37,1.05]$, norm of $dr_{1}$ in the interval $[0.1, 1.05]$ divided in 3 segments. Mean values of each box are shown in the first row in different colors. Shown data corresponds to a section of the image scale \ref  {fig:se2_image_scale}, indicated by a gray arrow. As expected all of the error means increase with the of norm of $dr_1$, but the rate of growth is different for each method.}}{37}{figure.5.2}}
\newlabel{fig:se2_boxplot}{{5.2}{37}{Errors of the numerical methods for the computation of the Log-composition of $dr_{0} \oplus dr_{1}$ in $\mathfrak {se}(2)$. Norm of $dr_{0}$ is in the interval $[0.37,1.05]$, norm of $dr_{1}$ in the interval $[0.1, 1.05]$ divided in 3 segments. Mean values of each box are shown in the first row in different colors. Shown data corresponds to a section of the image scale \ref {fig:se2_image_scale}, indicated by a gray arrow. As expected all of the error means increase with the of norm of $dr_1$, but the rate of growth is different for each method}{figure.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Log-composition for SVF}{37}{section.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Random generated vector field before and after the Gaussian smoother: in the first row a random generated vector field of dimension $50\times 50 \times 2$ where the vector values at each pixel are sampled from a random variable with normal distribution of mean $0$ and sigma $4$. The second row shows the same random vector field after a Gaussian smoothing of sigma $2$ (the code is based on the scipy library ndimage.filters.gaussian\textunderscore filter). Yhe last column shows the quiver of the vector field in the squared subregion of size $10\times 10$ at the point $(20,20)$. From the colorscale it is also possible to see that the values distribution of the filtered image have lost its symmetry. }}{38}{figure.5.3}}
\newlabel{fig:SVF_gaussian_smoothing_effects}{{5.3}{38}{Random generated vector field before and after the Gaussian smoother: in the first row a random generated vector field of dimension $50\times 50 \times 2$ where the vector values at each pixel are sampled from a random variable with normal distribution of mean $0$ and sigma $4$. The second row shows the same random vector field after a Gaussian smoothing of sigma $2$ (the code is based on the scipy library ndimage.filters.gaussian\textunderscore filter). Yhe last column shows the quiver of the vector field in the squared subregion of size $10\times 10$ at the point $(20,20)$. From the colorscale it is also possible to see that the values distribution of the filtered image have lost its symmetry}{figure.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Methods: random generated SVF}{38}{subsection.5.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Relationship between the initial standard deviation $\sigma _{\text  {init}}$ that defines the random SVF (stationary velocity field), the standard deviation of the Gaussian filter $\sigma _{\text  {gf}}$ utilized to regularize the SVF and its norm. Figure (a) represents schematically the two factors that define the norm of an SVF and with the blue dots we emphasized the values that has been chosen for the numerical computations proposed in (b) and (c). Figure (b) shows the mean of the norm of $10$ random generated SVF, as element of a Lie group, with initial standard deviation $\sigma _{\text  {init}}$ (on the x-axis) and Gaussian filter with standard deviation $\sigma _{\text  {gf}}$ (in different colors). Figure (c) shows the norm of the same element after exponentiating and so after having them in the Lie algebra. It is important to remark that it is not possible in general define a norm on a group. Nevertheless for matrices and for SVF it is possible to extend the norm from the Lie algebra to the Lie group, as proposed in chapter \ref  {ch:spatial_transformations} with the definition of displacement field norm (DS-norm). }}{39}{figure.5.4}}
\newlabel{fig:SVF_sigma_means_comparisons}{{5.4}{39}{Relationship between the initial standard deviation $\sigma _{\text {init}}$ that defines the random SVF (stationary velocity field), the standard deviation of the Gaussian filter $\sigma _{\text {gf}}$ utilized to regularize the SVF and its norm. Figure (a) represents schematically the two factors that define the norm of an SVF and with the blue dots we emphasized the values that has been chosen for the numerical computations proposed in (b) and (c). Figure (b) shows the mean of the norm of $10$ random generated SVF, as element of a Lie group, with initial standard deviation $\sigma _{\text {init}}$ (on the x-axis) and Gaussian filter with standard deviation $\sigma _{\text {gf}}$ (in different colors). Figure (c) shows the norm of the same element after exponentiating and so after having them in the Lie algebra. It is important to remark that it is not possible in general define a norm on a group. Nevertheless for matrices and for SVF it is possible to extend the norm from the Lie algebra to the Lie group, as proposed in chapter \ref {ch:spatial_transformations} with the definition of displacement field norm (DS-norm)}{figure.5.4}{}}
\newlabel{eq:angular_coefficients_for_the_gf}{{5.1}{39}{Methods: random generated SVF}{equation.5.2.1}{}}
\citation{arsigny2006log}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Mean errors for the numerical computation of the Log-composition of randomly generated stationary velocity fields (SVF). Initial standard deviations of the SVF $\sigma _{\text  {init}}^{0}$ and $\sigma _{\text  {init}}^{1}$ are given by the values on the axis for each sampling of $15$ elements. The error is computed without a ground truth according to the formula \ref  {eq:error_svf_sythetic_data}. When the norm of $\mathbf  {u}_1$ is small (see figure \ref  {fig:SVF_sigma_means_comparisons} to infer the norm from the standard deviations), parallel transport method and truncated BCH of degree 1 have comparable results, but parallel transport, as expected from the formula, is not symmetric respect to the size of the input vectors. Results of another sampling with the value of $\sigma _{\text  {init}}^{0}$ and $\sigma _{\text  {init}}^{1}$ are shown in figure \ref  {fig:SVF_scatter_plot}.}}{40}{figure.5.5}}
\newlabel{fig:SVF_image_scale}{{5.5}{40}{Mean errors for the numerical computation of the Log-composition of randomly generated stationary velocity fields (SVF). Initial standard deviations of the SVF $\sigma _{\text {init}}^{0}$ and $\sigma _{\text {init}}^{1}$ are given by the values on the axis for each sampling of $15$ elements. The error is computed without a ground truth according to the formula \ref {eq:error_svf_sythetic_data}. When the norm of $\mathbf {u}_1$ is small (see figure \ref {fig:SVF_sigma_means_comparisons} to infer the norm from the standard deviations), parallel transport method and truncated BCH of degree 1 have comparable results, but parallel transport, as expected from the formula, is not symmetric respect to the size of the input vectors. Results of another sampling with the value of $\sigma _{\text {init}}^{0}$ and $\sigma _{\text {init}}^{1}$ are shown in figure \ref {fig:SVF_scatter_plot}}{figure.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Log-composition for synthetic SVF}{40}{subsection.5.2.2}}
\newlabel{eq:error_svf_sythetic_data}{{5.4}{40}{Log-composition for synthetic SVF}{equation.5.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Comparisons of the errors of numerical computation of $\mathbf  {u}_0\oplus \mathbf  {u}_1$ with the method of truncated BCH of degree 0,1 and parallel transport. Parameters' values of the random generated SVF are schematically represented in figure (a). A set of $90$ SVF $\mathbf  {u}_0$ are generated with fixed parameters $\sigma _{\text  {gf}}^{0} = 2.0$ and $\sigma _{\text  {init}}^{0} = 1.0$; a second set of $90$ SVF $\mathbf  {u}_1$, are generated with the parameters $\sigma _{\text  {gf}}^{1} = 2.0$ and $\sigma _{\text  {init}}^{1}$ uniformly scattered in the the interval $(0.0, 2.0)$. On the x-axis fo figure (b) is shown the value of the resulting norm of $\mathbf  {u}_1$ for the chosen parameters, while on the y-axes are shown the values of the error for the numerical computation of the Log-composition $\mathbf  {u}_0\oplus \mathbf  {u}_1$ for each of the chosen methods. }}{41}{figure.5.6}}
\newlabel{fig:SVF_scatter_plot}{{5.6}{41}{Comparisons of the errors of numerical computation of $\mathbf {u}_0\oplus \mathbf {u}_1$ with the method of truncated BCH of degree 0,1 and parallel transport. Parameters' values of the random generated SVF are schematically represented in figure (a). A set of $90$ SVF $\mathbf {u}_0$ are generated with fixed parameters $\sigma _{\text {gf}}^{0} = 2.0$ and $\sigma _{\text {init}}^{0} = 1.0$; a second set of $90$ SVF $\mathbf {u}_1$, are generated with the parameters $\sigma _{\text {gf}}^{1} = 2.0$ and $\sigma _{\text {init}}^{1}$ uniformly scattered in the the interval $(0.0, 2.0)$. On the x-axis fo figure (b) is shown the value of the resulting norm of $\mathbf {u}_1$ for the chosen parameters, while on the y-axes are shown the values of the error for the numerical computation of the Log-composition $\mathbf {u}_0\oplus \mathbf {u}_1$ for each of the chosen methods}{figure.5.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Truncated BCH formula: The problem of the Jacobian matrix.}{41}{subsection.5.2.3}}
\newlabel{se:jacobian_problem}{{5.2.3}{41}{Truncated BCH formula: The problem of the Jacobian matrix}{subsection.5.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Relationship between the standard deviation of the Gaussian smoother that generates the SVF and the norm of the Lie bracket. Each square contains the means of $10$ Lie bracket (left) or nested Lie bracket (right) generated with initial standard deviation equals to $2$ and standard deviation of the gaussian smoother $\sigma _{\text  {gf}}$ indicated on the axes.}}{42}{figure.5.7}}
\newlabel{fig:SVF_image_scale_bracket_versus_gaussian}{{5.7}{42}{Relationship between the standard deviation of the Gaussian smoother that generates the SVF and the norm of the Lie bracket. Each square contains the means of $10$ Lie bracket (left) or nested Lie bracket (right) generated with initial standard deviation equals to $2$ and standard deviation of the gaussian smoother $\sigma _{\text {gf}}$ indicated on the axes}{figure.5.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Boxplot to compare the error between truncated BCH methods of degree $0$, $1$, $1.5$ and $2$. The size of each box is $100$ and the approximation of $\mathbf  {u}_{0}\oplus \mathbf  {u}_1$ is performed with $\delimiter "026A30C \delimiter 69640972 \mathbf  {u}_{0} \delimiter 86418188 \delimiter "026A30C  = 1.0$ and $\delimiter "026A30C \delimiter 69640972 \mathbf  {u}_{0} \delimiter 86418188 \delimiter "026A30C  = 0.1$. The standard deviation of the Gaussian filter $\sigma _{\text  {gf}}$ belongs to the set $(1.0, 2.0, 3.0, 4.0)$ and the initial standard deviation is computed such that $\sigma _{\text  {init}} = \delimiter "026A30C \delimiter 69640972 \mathbf  {u} \delimiter 86418188 \delimiter "026A30C /m_{\text  {alg}}(\sigma _{\text  {gf}})$ according to the formula \ref  {eq:angular_coefficients_for_the_gf}. With this strategy we have been able to compare vector of constant norm generated with increasing values for $\sigma _{\text  {gf}}$. The numbers written in blue above each box represents the mean value of the errors. For small $\sigma _{\text  {gf}}$, an increase in the order of the approximation does not always corresponds to a decrease in the error, and in general there no great improvements can be registered when the degree is greater than 1. }}{43}{figure.5.8}}
\newlabel{fig:SVF_boxplot_comparisons_BCH}{{5.8}{43}{Boxplot to compare the error between truncated BCH methods of degree $0$, $1$, $1.5$ and $2$. The size of each box is $100$ and the approximation of $\mathbf {u}_{0}\oplus \mathbf {u}_1$ is performed with $\euclideanMetric {\mathbf {u}_{0}} = 1.0$ and $\euclideanMetric {\mathbf {u}_{0}} = 0.1$. The standard deviation of the Gaussian filter $\sigma _{\text {gf}}$ belongs to the set $(1.0, 2.0, 3.0, 4.0)$ and the initial standard deviation is computed such that $\sigma _{\text {init}} = \euclideanMetric {\mathbf {u}}/m_{\text {alg}}(\sigma _{\text {gf}})$ according to the formula \ref {eq:angular_coefficients_for_the_gf}. With this strategy we have been able to compare vector of constant norm generated with increasing values for $\sigma _{\text {gf}}$. The numbers written in blue above each box represents the mean value of the errors. For small $\sigma _{\text {gf}}$, an increase in the order of the approximation does not always corresponds to a decrease in the error, and in general there no great improvements can be registered when the degree is greater than 1}{figure.5.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}A Problem for Three Brains}{43}{section.5.3}}
\newlabel{se:three_brains}{{5.3}{43}{A Problem for Three Brains}{section.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Design of Experiment}{43}{subsection.5.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces design of the experiment with SVF obtained with real data. Three figures represents three longitudinal scans of the same patient at time $A$, $B$ and $C$. The vector $\mathbf  {u}_{A}^{B}$ is the SVF that corresponds to the non-rigid transformation from $A$ to $B$ that aligns the image $B$ with $A$. Equivalently $\mathbf  {u}_{B}^{C}$ is the SVF from $B$ to $C$, and $\mathbf  {u}_{A}^{C}$ from $A$ to $C$. For the ideal registration algorithm we have that $\qopname  \relax o{log}(\qopname  \relax o{exp}(\mathbf  {u}_{B}^{C}) \circ \qopname  \relax o{exp}(\mathbf  {u}_{A}^{B}))$ $=$ $\mathbf  {u}_{A}^{C} $; when the first member is compute with a numerical method the difference with the second member members provides a measure of the error in the numerical method for the log-composition. When the registration is not ideal, the equation \ref  {eq:error_real_data_formula} (where M is the chosen numerical method) still provides still a consistent measure of the error of the different numerical methods for the computation of the the log composition. }}{44}{figure.5.9}}
\newlabel{fig:three_brains_problem}{{5.9}{44}{design of the experiment with SVF obtained with real data. Three figures represents three longitudinal scans of the same patient at time $A$, $B$ and $C$. The vector $\mathbf {u}_{A}^{B}$ is the SVF that corresponds to the non-rigid transformation from $A$ to $B$ that aligns the image $B$ with $A$. Equivalently $\mathbf {u}_{B}^{C}$ is the SVF from $B$ to $C$, and $\mathbf {u}_{A}^{C}$ from $A$ to $C$. For the ideal registration algorithm we have that $\log (\exp (\mathbf {u}_{B}^{C}) \circ \exp (\mathbf {u}_{A}^{B}))$ $=$ $\mathbf {u}_{A}^{C} $; when the first member is compute with a numerical method the difference with the second member members provides a measure of the error in the numerical method for the log-composition. When the registration is not ideal, the equation \ref {eq:error_real_data_formula} (where M is the chosen numerical method) still provides still a consistent measure of the error of the different numerical methods for the computation of the the log composition}{figure.5.9}{}}
\newlabel{eq:transitivity_of_the_registration}{{5.5}{44}{Design of Experiment}{equation.5.3.5}{}}
\newlabel{eq:error_real_data_formula}{{5.6}{44}{Design of Experiment}{equation.5.3.6}{}}
\citation{jack2008alzheimer}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Results}{45}{subsection.5.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Log-composition for a dataset of 16 MRI T1 brain images scan of control subjects. Three longitudinal scans $A$, $B$ and $C$ are available for each subject, and corresponds to a scan at time $0$, after three months and after $6$ month from the $0$. The design of the experiment is shown in figure \ref  {fig:three_brains_problem}, and the error of the log-composition is computed with the formula \ref  {eq:error_real_data_formula}, where $\mathbf  {u}_{A}^{B}$ is the SVF that corresponds to the non-rigid transformation from $A$ to $B$; $M{\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ }\mathbf  {u}_{B}^{C} \oplus \mathbf  {u}_{A}^{B} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$ is the numerical computation of the log composition $\mathbf  {u}_{B}^{C} \oplus \mathbf  {u}_{A}^{B}$ with the method $M$, chosen among $\text  {BCH}^0$, $\text  {BCH}^1$, $\text  {BCH}^{1.5}$, $\text  {BCH}^2$ and parallel transport (P.T.).}}{45}{figure.5.10}}
\newlabel{fig:svf_log_composition_boxplot_real_data_CTL}{{5.10}{45}{Log-composition for a dataset of 16 MRI T1 brain images scan of control subjects. Three longitudinal scans $A$, $B$ and $C$ are available for each subject, and corresponds to a scan at time $0$, after three months and after $6$ month from the $0$. The design of the experiment is shown in figure \ref {fig:three_brains_problem}, and the error of the log-composition is computed with the formula \ref {eq:error_real_data_formula}, where $\mathbf {u}_{A}^{B}$ is the SVF that corresponds to the non-rigid transformation from $A$ to $B$; $M\big (\mathbf {u}_{B}^{C} \oplus \mathbf {u}_{A}^{B} \big )$ is the numerical computation of the log composition $\mathbf {u}_{B}^{C} \oplus \mathbf {u}_{A}^{B}$ with the method $M$, chosen among $\text {BCH}^0$, $\text {BCH}^1$, $\text {BCH}^{1.5}$, $\text {BCH}^2$ and parallel transport (P.T.)}{figure.5.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Lie Logarithm computation for $SE(2)$}{46}{section.5.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Number of steps required to obtain convergence in for different methods utilized in the logarithm computation algorithm. The data set contains $200$ random matrices in the Lie group $SE(2)$, with Frobenius norm between $1$ and $3$. On the top it is possible to visualize the mean number of step to reach the convergence for each method.}}{46}{figure.5.11}}
\newlabel{fig:log_computation_boxplot}{{5.11}{46}{Number of steps required to obtain convergence in for different methods utilized in the logarithm computation algorithm. The data set contains $200$ random matrices in the Lie group $SE(2)$, with Frobenius norm between $1$ and $3$. On the top it is possible to visualize the mean number of step to reach the convergence for each method}{figure.5.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Empirical Evaluations of the Computational Time}{46}{section.5.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces Relationship between the size of the figure (x-axes) and the computational time for a data set of $20$ random generated SVF. The yellow line labeled with ground represents the time of the computation of $\qopname  \relax o{exp}{\mathbf  {u}_0}\circ \qopname  \relax o{exp}{\mathbf  {u}_1}$, while the other represents the exponentiation of the numerical method for the computation of the log-composition.}}{47}{figure.5.12}}
\newlabel{fig:svf_computational_time}{{5.12}{47}{Relationship between the size of the figure (x-axes) and the computational time for a data set of $20$ random generated SVF. The yellow line labeled with ground represents the time of the computation of $\exp {\mathbf {u}_0}\circ \exp {\mathbf {u}_1}$, while the other represents the exponentiation of the numerical method for the computation of the log-composition}{figure.5.12}{}}
\@setckpt{chapters/Ch_results}{
\setcounter{page}{48}
\setcounter{equation}{6}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{5}
\setcounter{section}{5}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{12}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{definition}{0}
\setcounter{example}{0}
\setcounter{lemma}{0}
\setcounter{observation}{0}
\setcounter{theorem}{0}
\setcounter{prop}{0}
\setcounter{algorithm}{0}
\setcounter{Item}{39}
\setcounter{Hfootnote}{1}
\setcounter{bookmark@seq@number}{41}
\setcounter{section@level}{1}
}
